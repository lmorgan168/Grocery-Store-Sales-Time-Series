{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "golden-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "radio-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gpu\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "biological-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    'train.csv',\n",
    "    usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={\n",
    "        'store_nbr': 'category',\n",
    "        'family': 'category',\n",
    "        'sales': 'float32',\n",
    "        'onpromotion': 'uint32',\n",
    "    },\n",
    "    parse_dates=['date'],\n",
    "    infer_datetime_format=True,\n",
    ")\n",
    "\n",
    "test_df =  pd.read_csv(\n",
    "    'test.csv',\n",
    "    dtype={\n",
    "        'store_nbr': 'category',\n",
    "        'family': 'category',\n",
    "        'onpromotion': 'uint32',\n",
    "    },\n",
    "    parse_dates=['date'],\n",
    "    infer_datetime_format=True,\n",
    ")\n",
    "\n",
    "oil_df =  pd.read_csv(\n",
    "    'oil.csv',\n",
    "    parse_dates=['date'],\n",
    "    infer_datetime_format=True,\n",
    ") \n",
    "\n",
    "holidays_events_df = pd.read_csv(\n",
    "    'holidays_events.csv',\n",
    "    dtype={\n",
    "        'type': 'category',\n",
    "        'locale': 'category',\n",
    "        'locale_name': 'category',\n",
    "        'description': 'category',\n",
    "    },\n",
    "    parse_dates = ['date'],\n",
    "    infer_datetime_format = True,\n",
    ").rename(columns = {'type':'holiday_type'})\n",
    "\n",
    "stores_df = pd.read_csv(\n",
    "    'stores.csv',\n",
    "    dtype = {\n",
    "        'store_nbr': 'category',\n",
    "        'city': 'category',\n",
    "        'state': 'category',\n",
    "        'type': 'category',\n",
    "        'cluster': 'category'\n",
    "    }).rename(columns ={'type':'store_type'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "funded-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show holidays for only the relevant time periods\n",
    "holidays_events_df = holidays_events_df.loc[holidays_events_df['date'] < pd.to_datetime('2017/09/01')]  # less than last date in test data\n",
    "holidays_events_df = holidays_events_df.loc[holidays_events_df['date'] >= pd.to_datetime('2013/01/01')] # greater than or equal to first day in train\n",
    "\n",
    "\n",
    "## need to join the tables first as there are duplicate dates for holidays at different locales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "forbidden-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df = stores_df.drop(columns=['store_type', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "second-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(stores_df, on = 'store_nbr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-office",
   "metadata": {},
   "source": [
    "# Holiday/Store Data\n",
    "\n",
    "Ultimately, did not use the cells below account for holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "needed-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column affected_by_holiday with values np.nan\n",
    "# train_df['affected_by_holiday'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "graphic-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for loop to join , takes way too long as I don't know how to implement a better way\n",
    "\n",
    "#for i in range(holidays_events_df.shape[0]):\n",
    "#    j_index = np.where( (train_df['date'] == holidays_events_df.iloc[i].date) == True)[0]\n",
    "#    for j in j_index:\n",
    "#        if (train_df.city[j] == holidays_events_df.iloc[i].locale_name or train_df.state[j] == holidays_events_df.iloc[i].locale_name):\n",
    "#            train_df['affected_by_holiday'].iloc[j] = True\n",
    "#        else:\n",
    "#            train_df['affected_by_holiday'].iloc[j] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "verified-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df = pd.concat([train_df, test_df], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fitted-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df = full_df.merge(stores_df, how = 'left', on = 'store_nbr')\n",
    "#full_df = full_df.merge(oil_df, how = 'left', on = 'date')\n",
    "#full_df = full_df.merge(holidays_events_df, how = 'left', on = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-peoples",
   "metadata": {},
   "source": [
    "# Filling NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "minus-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "## oil is missing dates because trading does not happen on weekends, but we we need this to match our data for the LSTM\n",
    "## so we can interpret the weekends oil price with interpolate\n",
    "\n",
    "# fill missing dates with settting date as index and reindexing with date range from beginning of train set to end of test set\n",
    "idx = pd.date_range(oil_df.date.min(),oil_df.date.max())\n",
    "oil_df = oil_df.set_index('date').reindex(idx)\n",
    "\n",
    "# fill empty oil prices using interpolation with method time\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].interpolate(method = 'time', limit_direction='both')\n",
    "oil_df = oil_df.rename_axis('date').reset_index()    # make date a column again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "seeing-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing without holiday events\n",
    "\n",
    "\n",
    "train_df = train_df.merge(oil_df, on = 'date')\n",
    "test_df = test_df.merge(oil_df, on=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "boolean-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df['dcoilwtico'] = full_df['dcoilwtico'].interpolate(limit_direction='both')\n",
    "#full_df['holiday_type'] = full_df['holiday_type'].fillna('Work Day')\n",
    "#full_df['transferred'] = full_df['transferred'].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-charlotte",
   "metadata": {},
   "source": [
    "# Converting data to supervised learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "reserved-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, futureArr=None, targetCol=None, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s(t-%d)' % (j, i)) for j in df.columns]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)    \n",
    "    if futureArr != None:\n",
    "        for i in range(0, n_out):\n",
    "            for futureCol in futureArr:\n",
    "                cols.append(df.shift(-i)[futureCol])\n",
    "                if i == 0:\n",
    "                    names += [('%s(t)' % (futureCol))]\n",
    "                else:\n",
    "                    names += [('%s(t+%d)' % (futureCol, i))]\n",
    "    \n",
    "    for i in range(0, n_out):\n",
    "        if targetCol == None:\n",
    "            cols.append(df.shift(-i))\n",
    "            if i == 0:\n",
    "                names += [('%s(t)' % (j)) for j in df.columns]\n",
    "            else:\n",
    "                names += [('%s(t+%d)' % (j, i)) for j in df.columns]\n",
    "        else:\n",
    "            cols.append(df.shift(-i)[targetCol])\n",
    "            if i == 0:\n",
    "                names += [('%s(t)' % (targetCol))]\n",
    "            else:\n",
    "                names += [('%s(t+%d)' % (targetCol, i))]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "grand-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_nbrs = train_df['store_nbr'].unique()\n",
    "family_types = train_df['family'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "allied-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_nbr_type in store_nbrs:\n",
    "    for family_type in family_types:\n",
    "        \n",
    "        train_df1 = train_df[(train_df[\"store_nbr\"] == store_nbr_type) & (train_df[\"family\"] == family_type)]\n",
    "        \n",
    "        train_df1 = train_df1.reset_index()\n",
    "        train_df1 = train_df1.drop(columns = ['index', \"date\", \"store_nbr\", \"family\"])\n",
    "        \n",
    "   \n",
    "        test_df1 = test_df[(test_df[\"store_nbr\"] == store_nbr_type) & (test_df[\"family\"] == family_type)]\n",
    "        test_df1 = test_df1.drop(columns = [\"date\", \"store_nbr\", \"family\"])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "vocal-console",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>92.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.146667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>46.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21384</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>46.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23166</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>46.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24948</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>45.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>47.260000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sales  onpromotion  dcoilwtico\n",
       "0        0.0            0   93.140000\n",
       "1        2.0            0   93.140000\n",
       "2        3.0            0   92.970000\n",
       "3        3.0            0   93.120000\n",
       "4        5.0            0   93.146667\n",
       "...      ...          ...         ...\n",
       "19602    NaN            0   46.816667\n",
       "21384    NaN            0   46.400000\n",
       "23166    NaN            0   46.460000\n",
       "24948    NaN            0   45.960000\n",
       "26730    NaN            0   47.260000\n",
       "\n",
       "[1700 rows x 3 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.concat([train_df1, test_df1]).drop(columns=['id', 'city', 'state'])\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "hydraulic-nightmare",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sales  onpromotion  dcoilwtico\n",
       "0     0.000000          0.0    0.792965\n",
       "1     0.105263          0.0    0.792965\n",
       "2     0.157895          0.0    0.790951\n",
       "3     0.157895          0.0    0.792728\n",
       "4     0.263158          0.0    0.793044\n",
       "...        ...          ...         ...\n",
       "1695       NaN          0.0    0.244305\n",
       "1696       NaN          0.0    0.239370\n",
       "1697       NaN          0.0    0.240081\n",
       "1698       NaN          0.0    0.234158\n",
       "1699       NaN          0.0    0.249556\n",
       "\n",
       "[1700 rows x 3 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = full_df.columns\n",
    "\n",
    "\n",
    "## scales all the data in each column 0 to 1\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "full_df = scaler.fit_transform(full_df)\n",
    "\n",
    "full_df=pd.DataFrame(full_df, columns = features)\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "multiple-physics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales(t-5)</th>\n",
       "      <th>onpromotion(t-5)</th>\n",
       "      <th>dcoilwtico(t-5)</th>\n",
       "      <th>sales(t-4)</th>\n",
       "      <th>onpromotion(t-4)</th>\n",
       "      <th>dcoilwtico(t-4)</th>\n",
       "      <th>sales(t-3)</th>\n",
       "      <th>onpromotion(t-3)</th>\n",
       "      <th>dcoilwtico(t-3)</th>\n",
       "      <th>sales(t-2)</th>\n",
       "      <th>...</th>\n",
       "      <th>dcoilwtico(t-2)</th>\n",
       "      <th>sales(t-1)</th>\n",
       "      <th>onpromotion(t-1)</th>\n",
       "      <th>dcoilwtico(t-1)</th>\n",
       "      <th>onpromotion(t)</th>\n",
       "      <th>dcoilwtico(t)</th>\n",
       "      <th>onpromotion(t+1)</th>\n",
       "      <th>dcoilwtico(t+1)</th>\n",
       "      <th>sales(t)</th>\n",
       "      <th>sales(t+1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792965</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792965</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790951</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792728</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793675</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792965</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790951</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792728</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793044</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.790951</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792728</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793044</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792254</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792728</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793044</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793675</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793044</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793675</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793794</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798413</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276126</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275337</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274547</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270994</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267914</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275337</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274547</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270994</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277153</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263098</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274547</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270994</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277153</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264716</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258281</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270994</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277153</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264716</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267914</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253464</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277153</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264716</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267914</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263098</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253228</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1678 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sales(t-5)  onpromotion(t-5)  dcoilwtico(t-5)  sales(t-4)  \\\n",
       "5       0.000000               0.0         0.792965    0.105263   \n",
       "6       0.105263               0.0         0.792965    0.157895   \n",
       "7       0.157895               0.0         0.790951    0.157895   \n",
       "8       0.157895               0.0         0.792728    0.263158   \n",
       "9       0.263158               0.0         0.793044    0.105263   \n",
       "...          ...               ...              ...         ...   \n",
       "1678    0.263158               0.0         0.276126    0.315789   \n",
       "1679    0.315789               0.0         0.275337    0.368421   \n",
       "1680    0.368421               0.0         0.274547    0.210526   \n",
       "1681    0.210526               0.0         0.270994    0.368421   \n",
       "1682    0.368421               0.0         0.277153    0.473684   \n",
       "\n",
       "      onpromotion(t-4)  dcoilwtico(t-4)  sales(t-3)  onpromotion(t-3)  \\\n",
       "5                  0.0         0.792965    0.157895               0.0   \n",
       "6                  0.0         0.790951    0.157895               0.0   \n",
       "7                  0.0         0.792728    0.263158               0.0   \n",
       "8                  0.0         0.793044    0.105263               0.0   \n",
       "9                  0.0         0.793359    0.000000               0.0   \n",
       "...                ...              ...         ...               ...   \n",
       "1678               0.0         0.275337    0.368421               0.0   \n",
       "1679               0.0         0.274547    0.210526               0.0   \n",
       "1680               0.0         0.270994    0.368421               0.0   \n",
       "1681               0.0         0.277153    0.473684               0.0   \n",
       "1682               0.0         0.264716    0.052632               0.0   \n",
       "\n",
       "      dcoilwtico(t-3)  sales(t-2)  ...  dcoilwtico(t-2)  sales(t-1)  \\\n",
       "5            0.790951    0.157895  ...         0.792728    0.263158   \n",
       "6            0.792728    0.263158  ...         0.793044    0.105263   \n",
       "7            0.793044    0.105263  ...         0.793359    0.000000   \n",
       "8            0.793359    0.000000  ...         0.793675    0.105263   \n",
       "9            0.793675    0.105263  ...         0.793794    0.105263   \n",
       "...               ...         ...  ...              ...         ...   \n",
       "1678         0.274547    0.210526  ...         0.270994    0.368421   \n",
       "1679         0.270994    0.368421  ...         0.277153    0.473684   \n",
       "1680         0.277153    0.473684  ...         0.264716    0.052632   \n",
       "1681         0.264716    0.052632  ...         0.267914    0.315789   \n",
       "1682         0.267914    0.315789  ...         0.263098    0.052632   \n",
       "\n",
       "      onpromotion(t-1)  dcoilwtico(t-1)  onpromotion(t)  dcoilwtico(t)  \\\n",
       "5                  0.0         0.793044             0.0       0.793359   \n",
       "6                  0.0         0.793359             0.0       0.793675   \n",
       "7                  0.0         0.793675             0.0       0.793794   \n",
       "8                  0.0         0.793794             0.0       0.792254   \n",
       "9                  0.0         0.792254             0.0       0.800900   \n",
       "...                ...              ...             ...            ...   \n",
       "1678               0.0         0.277153             0.0       0.264716   \n",
       "1679               0.0         0.264716             0.0       0.267914   \n",
       "1680               0.0         0.267914             0.0       0.263098   \n",
       "1681               0.0         0.263098             0.0       0.258281   \n",
       "1682               0.0         0.258281             0.0       0.253464   \n",
       "\n",
       "      onpromotion(t+1)  dcoilwtico(t+1)  sales(t)  sales(t+1)  \n",
       "5                  0.0         0.793675  0.105263    0.000000  \n",
       "6                  0.0         0.793794  0.000000    0.105263  \n",
       "7                  0.0         0.792254  0.105263    0.105263  \n",
       "8                  0.0         0.800900  0.105263    0.105263  \n",
       "9                  0.0         0.798413  0.105263    0.157895  \n",
       "...                ...              ...       ...         ...  \n",
       "1678               0.0         0.267914  0.473684    0.052632  \n",
       "1679               0.0         0.263098  0.052632    0.315789  \n",
       "1680               0.0         0.258281  0.315789    0.052632  \n",
       "1681               0.0         0.253464  0.052632    0.052632  \n",
       "1682               0.0         0.253228  0.052632    0.210526  \n",
       "\n",
       "[1678 rows x 21 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futureArr = ['onpromotion', 'dcoilwtico']\n",
    "\n",
    "series_to_supervised(full_df, 5, 2, futureArr = futureArr, targetCol = 'sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-change",
   "metadata": {},
   "source": [
    "# Splitting the Data\n",
    "\n",
    "It is not trivial to perform cross-validation on time-series data as it does not make sense to use future values to forecast past values. \n",
    "\n",
    "Thus we will perform a one step ahead cross validation as described by Hyndman and Athanasopoulos. We will perform the model using different numbers of 'past_days'. We will try 7, 14, 30, 50, and 100 as the number of past days.\n",
    "\n",
    "Because our approach is fitting different models for different combinations of store_nbr and family, we will only look at store_nbr 1 and family Automotive for learning purposes.\n",
    "\n",
    "\n",
    "for past_days = 8, we had mse: 0.0190 in the training and val_mse: 0.0218 for validation\n",
    "\n",
    "for past_days = 16, we had mse: 0.0190 for training and val_mse: 0.0218 for validation\n",
    "\n",
    "for past_days = 32, we had mse: 0.0192 for training and val_mse: 0.0214 for validation\n",
    "\n",
    "for past_days = 48, we had mse: 0.0194 for training and val_mse: 0.0214 for validation\n",
    "\n",
    "for past_days = 100, we had mse: 0.0196 for training and val_mse: 0.0213 for validation\n",
    "\n",
    "\n",
    "seeing these trends, we can say that ultimately, training using a higher number of past_days does not provide much benefit but does increase the number of trainable parameters. As such, we will stay with a smaller past_days. \n",
    "\n",
    "\n",
    "We will also opt to check the batch_size and the number of epochs ranging from (20, 150) for batch_size and (20,100) for epochs.\n",
    "\n",
    "batch_size 50, epochs 20 = loss: 0.0189 - mse: 0.0189 - val_loss: 0.0221 - val_mse: 0.0220\n",
    "\n",
    "batch_size 100, epochs 20 = loss: 0.0191 - mse: 0.0191 - val_loss: 0.0220 - val_mse: 0.0216\n",
    "\n",
    "batch_size 150, epochs 20 = loss: 0.0191 - mse: 0.0189 - val_loss: 0.0220 - val_mse: 0.0214\n",
    "\n",
    "batch_size 200, epochs 20 = loss: 0.0192 - mse: 0.0190 - val_loss: 0.0221 - val_mse: 0.0219\n",
    "\n",
    "batch_size 150, epochs 50 =  loss: 0.0190 - mse: 0.0189 - val_loss: 0.0220 - val_mse: 0.0214\n",
    "\n",
    "batch_size 150, epochs 100 = loss: 0.0190 - mse: 0.0189 - val_loss: 0.0221 - val_mse: 0.0215\n",
    "\n",
    "Ranging the epochs did not have much of an effect. Even if we remove the stopping conditions, there is not enough variable change as the stopping condition already accounts for this, so keeping epochs at 20 is reasonable. We seem to have the best loss with batch_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "cosmetic-mambo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1328, 1, 56) (1328, 16) (333, 1, 56) (333, 16) (1, 1, 56)\n"
     ]
    }
   ],
   "source": [
    "past_days = 8\n",
    "predict_days = 16\n",
    "\n",
    "futureArr = ['onpromotion', 'dcoilwtico']\n",
    "targetCol = 'sales'\n",
    "\n",
    "train = series_to_supervised(full_df, past_days, predict_days, futureArr, targetCol)\n",
    "\n",
    "values = train.values\n",
    "\n",
    "\n",
    "## split into train and validation sets\n",
    "\n",
    "split_ratio = 0.8\n",
    "\n",
    "split_number = np.floor(len(train.index) * split_ratio)\n",
    "split_number = np.int(split_number)\n",
    "        \n",
    "train = values[:split_number, :]\n",
    "val = values[split_number:, :]\n",
    "\n",
    "## split into input and outputs\n",
    "\n",
    "train_x, train_y = train[:, :-predict_days], train[:, -predict_days:]\n",
    "val_x, val_y = val[:, :-predict_days], val[:, -predict_days:]\n",
    "\n",
    "## reshape input to be 3D [samples, timesteps, features]\n",
    "\n",
    "train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "val_x = val_x.reshape((val_x.shape[0], 1, val_x.shape[1]))\n",
    "\n",
    "\n",
    "prediction_data = series_to_supervised(full_df, past_days, predict_days, futureArr, targetCol, dropnan=False).values[-17:-16, :-predict_days]\n",
    "prediction_data = prediction_data.reshape((prediction_data.shape[0], 1, prediction_data.shape[1]))\n",
    "\n",
    "print(train_x.shape, train_y.shape, val_x.shape, val_y.shape, prediction_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "proud-inflation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 1, 30)             10440     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1, 30)             0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 1, 30)             7320      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 1, 30)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 1, 16)             496       \n",
      "=================================================================\n",
      "Total params: 18,256\n",
      "Trainable params: 18,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = keras.models.Sequential([\n",
    "        keras.layers.LSTM(units=30, return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2])),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.LSTM(units=30, return_sequences=True),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(predict_days))\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "theoretical-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 - 2s - loss: 0.0427 - mse: 0.0423 - val_loss: 0.0567 - val_mse: 0.0550\n",
      "Epoch 2/100\n",
      "9/9 - 0s - loss: 0.0360 - mse: 0.0358 - val_loss: 0.0481 - val_mse: 0.0466\n",
      "Epoch 3/100\n",
      "9/9 - 0s - loss: 0.0315 - mse: 0.0313 - val_loss: 0.0411 - val_mse: 0.0398\n",
      "Epoch 4/100\n",
      "9/9 - 0s - loss: 0.0294 - mse: 0.0292 - val_loss: 0.0370 - val_mse: 0.0358\n",
      "Epoch 5/100\n",
      "9/9 - 0s - loss: 0.0276 - mse: 0.0274 - val_loss: 0.0344 - val_mse: 0.0333\n",
      "Epoch 6/100\n",
      "9/9 - 0s - loss: 0.0261 - mse: 0.0260 - val_loss: 0.0324 - val_mse: 0.0314\n",
      "Epoch 7/100\n",
      "9/9 - 0s - loss: 0.0250 - mse: 0.0248 - val_loss: 0.0305 - val_mse: 0.0296\n",
      "Epoch 8/100\n",
      "9/9 - 0s - loss: 0.0240 - mse: 0.0239 - val_loss: 0.0288 - val_mse: 0.0280\n",
      "Epoch 9/100\n",
      "9/9 - 0s - loss: 0.0232 - mse: 0.0231 - val_loss: 0.0273 - val_mse: 0.0266\n",
      "Epoch 10/100\n",
      "9/9 - 0s - loss: 0.0224 - mse: 0.0223 - val_loss: 0.0259 - val_mse: 0.0253\n",
      "Epoch 11/100\n",
      "9/9 - 0s - loss: 0.0218 - mse: 0.0217 - val_loss: 0.0248 - val_mse: 0.0243\n",
      "Epoch 12/100\n",
      "9/9 - 0s - loss: 0.0213 - mse: 0.0212 - val_loss: 0.0238 - val_mse: 0.0234\n",
      "Epoch 13/100\n",
      "9/9 - 0s - loss: 0.0210 - mse: 0.0209 - val_loss: 0.0232 - val_mse: 0.0228\n",
      "Epoch 14/100\n",
      "9/9 - 0s - loss: 0.0208 - mse: 0.0207 - val_loss: 0.0227 - val_mse: 0.0224\n",
      "Epoch 15/100\n",
      "9/9 - 0s - loss: 0.0205 - mse: 0.0204 - val_loss: 0.0224 - val_mse: 0.0221\n",
      "Epoch 16/100\n",
      "9/9 - 0s - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0222 - val_mse: 0.0220\n",
      "Epoch 17/100\n",
      "9/9 - 0s - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0221 - val_mse: 0.0219\n",
      "Epoch 18/100\n",
      "9/9 - 0s - loss: 0.0202 - mse: 0.0201 - val_loss: 0.0220 - val_mse: 0.0218\n",
      "Epoch 19/100\n",
      "9/9 - 0s - loss: 0.0202 - mse: 0.0201 - val_loss: 0.0220 - val_mse: 0.0218\n",
      "Epoch 20/100\n",
      "9/9 - 0s - loss: 0.0201 - mse: 0.0200 - val_loss: 0.0219 - val_mse: 0.0218\n",
      "Epoch 21/100\n",
      "9/9 - 0s - loss: 0.0200 - mse: 0.0199 - val_loss: 0.0219 - val_mse: 0.0217\n",
      "Epoch 22/100\n",
      "9/9 - 0s - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0219 - val_mse: 0.0217\n",
      "Epoch 23/100\n",
      "9/9 - 0s - loss: 0.0200 - mse: 0.0199 - val_loss: 0.0219 - val_mse: 0.0217\n",
      "Epoch 24/100\n",
      "9/9 - 0s - loss: 0.0199 - mse: 0.0198 - val_loss: 0.0219 - val_mse: 0.0217\n",
      "Epoch 25/100\n",
      "9/9 - 0s - loss: 0.0198 - mse: 0.0197 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 26/100\n",
      "9/9 - 0s - loss: 0.0199 - mse: 0.0198 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 27/100\n",
      "9/9 - 0s - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 28/100\n",
      "9/9 - 0s - loss: 0.0198 - mse: 0.0197 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 29/100\n",
      "9/9 - 0s - loss: 0.0197 - mse: 0.0196 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 30/100\n",
      "9/9 - 0s - loss: 0.0197 - mse: 0.0196 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 31/100\n",
      "9/9 - 0s - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 32/100\n",
      "9/9 - 0s - loss: 0.0196 - mse: 0.0195 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 33/100\n",
      "9/9 - 0s - loss: 0.0196 - mse: 0.0195 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 34/100\n",
      "9/9 - 0s - loss: 0.0196 - mse: 0.0195 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 35/100\n",
      "9/9 - 0s - loss: 0.0196 - mse: 0.0195 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 36/100\n",
      "9/9 - 0s - loss: 0.0195 - mse: 0.0194 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 37/100\n",
      "9/9 - 0s - loss: 0.0195 - mse: 0.0194 - val_loss: 0.0218 - val_mse: 0.0217\n",
      "Epoch 38/100\n",
      "9/9 - 0s - loss: 0.0195 - mse: 0.0194 - val_loss: 0.0219 - val_mse: 0.0217\n",
      "Epoch 39/100\n",
      "9/9 - 0s - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0219 - val_mse: 0.0218\n",
      "Epoch 40/100\n",
      "9/9 - 0s - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0219 - val_mse: 0.0218\n",
      "Epoch 41/100\n",
      "9/9 - 0s - loss: 0.0194 - mse: 0.0194 - val_loss: 0.0219 - val_mse: 0.0217\n",
      "Epoch 42/100\n",
      "9/9 - 0s - loss: 0.0194 - mse: 0.0193 - val_loss: 0.0219 - val_mse: 0.0217\n",
      "Epoch 43/100\n",
      "9/9 - 0s - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0219 - val_mse: 0.0218\n"
     ]
    }
   ],
   "source": [
    "early_stopping =  keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model_result = model.fit(train_x, train_y, epochs=100, batch_size=150, validation_data=(val_x, val_y), verbose=2, shuffle=False, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "integral-bryan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_result.history[\"loss\"], label=\"training\")\n",
    "plt.plot(model_result.history[\"val_loss\"], label=\"validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_result.history[\"mse\"], label=\"training\")\n",
    "plt.plot(model_result.history[\"val_mse\"], label=\"validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "described-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the sales(t+1 to t+16) for our prediction data (which is the last sample))\n",
    "prediction = model.predict(prediction_data)\n",
    "\n",
    "#rescale our outputs\n",
    "prediction = np.squeeze(prediction) / scaler.scale_[0]\n",
    "\n",
    "\n",
    "#apply the predicted data to our test_dataframe of the respective store_type and family type\n",
    "test_df1['sales'] = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-chester",
   "metadata": {},
   "source": [
    "# Looping through each store_nbr, family combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "visible-florence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "## putting it all together with each store_nbr and family\n",
    "\n",
    "store_nbr_types = train_df[\"store_nbr\"].unique()\n",
    "\n",
    "family_types = train_df[\"family\"].unique()\n",
    "\n",
    "counter = 0 ## counter used to visit all store/type combinations\n",
    "\n",
    "\n",
    "for store_nbr_type in store_nbr_types:\n",
    "    for family_type in family_types:\n",
    "        counter = counter + 1\n",
    "\n",
    "        train_data = train_df[(train_df[\"store_nbr\"] == store_nbr_type) & (train_df[\"family\"] == family_type)]\n",
    "        \n",
    "        # reset the index so it doesnt mess up with total concatenated data, then drop\n",
    "        train_data = train_data.reset_index()\n",
    "        train_data = train_data.drop(columns = [\"index\", \"date\", \"store_nbr\", \"family\"])\n",
    "        \n",
    "        # test data for the respective store,family combination to fill sales      \n",
    "        test_data = test_df[(test_df[\"store_nbr\"] == store_nbr_type) & (test_df[\"family\"] == family_type)]\n",
    "        test_data = test_data.drop(columns = [\"date\", \"store_nbr\", \"family\"])\n",
    "        \n",
    "        # concat train and test data\n",
    "        full_df = pd.concat([train_data, test_data]).drop(columns=[\"id\", 'city', 'state'])\n",
    "        \n",
    "        # Normalization\n",
    "        features = full_df.columns\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "        full_df = scaler.fit_transform(full_df)\n",
    "\n",
    "        full_df = pd.DataFrame(full_df, columns=feature_name)\n",
    "        \n",
    "        # number of days to forecast\n",
    "        past_days = 8\n",
    "        # number of days to predict\n",
    "        predict_days = 16\n",
    "        # what columns as features\n",
    "        futureArr = [\"onpromotion\", \"dcoilwtico\"]\n",
    "        # what column is target\n",
    "        targetCol = \"sales\"\n",
    "\n",
    "        train = series_to_supervised(full_df, past_days, predict_days, futureArr, targetCol)\n",
    "\n",
    "        split_ratio = 0.8\n",
    "\n",
    "        split_number = np.floor(len(train.index) * split_ratio)\n",
    "        split_number = np.int(split_number)\n",
    "\n",
    "        values = train.values\n",
    "\n",
    "\n",
    "        # split into train and validation sets\n",
    "        train = values[:split_number, :]\n",
    "        val = values[split_number:, :]\n",
    "\n",
    "        # split into input and outputs\n",
    "        train_x, train_y = train[:, :-predict_days], train[:, -predict_days:]\n",
    "        val_x, val_y = val[:, :-predict_days], val[:, -predict_days:]\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "        val_x = val_x.reshape((val_x.shape[0], 1, val_x.shape[1]))\n",
    "\n",
    "        # the use of (-17:-16 represents use ofdata in last 17 days)\n",
    "        prediction_data = series_to_supervised(full_df, past_days, predict_days, futureArr, targetCol, dropnan=False).values[-17:-16, :-predict_days]\n",
    "        prediction_data = prediction_data.reshape((prediction_data.shape[0], 1, prediction_data.shape[1]))\n",
    "        \n",
    "        model_result = model.fit(train_x, train_y, epochs=50, batch_size=150, validation_data=(val_x, val_y), verbose=0, shuffle=False, callbacks=[early_stopping])\n",
    "        \n",
    "        # Inference        \n",
    "        prediction = model.predict(prediction_data)\n",
    "\n",
    "        prediction = np.squeeze(prediction) / scaler.scale_[0]\n",
    "\n",
    "        test_data[\"sales\"] = prediction\n",
    "        \n",
    "        if counter == 1:\n",
    "            submit_data = test_data\n",
    "        else:\n",
    "            submit_data = pd.concat([submit_data, test_data])\n",
    "        \n",
    "        if counter % 50 == 0:\n",
    "            print(counter)\n",
    "        \n",
    "        \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-promotion",
   "metadata": {},
   "source": [
    "# Saving the data to submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "incorrect-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_data.to_csv('submit_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "lyric-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submit_data.drop(columns = ['dcoilwtico','onpromotion'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "precious-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
